{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf74d861-a5e3-4aec-bb87-242eee7ea0dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 9912422/9912422 [00:16<00:00, 615852.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 28881/28881 [00:00<00:00, 371597.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 1648877/1648877 [00:00<00:00, 2637392.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 4542/4542 [00:00<00:00, 1013595.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 48004/48004 [00:00<00:00, 51750.53it/s]\n",
      "100%|██████████████████████████████████| 10000/10000 [00:00<00:00, 50032.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.0504\n",
      "Epoch [2/100], Loss: 0.0339\n",
      "Epoch [3/100], Loss: 0.0302\n",
      "Epoch [4/100], Loss: 0.0268\n",
      "Epoch [5/100], Loss: 0.0246\n",
      "Epoch [6/100], Loss: 0.0227\n",
      "Epoch [7/100], Loss: 0.0208\n",
      "Epoch [8/100], Loss: 0.0183\n",
      "Epoch [9/100], Loss: 0.0180\n",
      "Epoch [10/100], Loss: 0.0170\n",
      "Epoch [11/100], Loss: 0.0141\n",
      "Epoch [12/100], Loss: 0.0139\n",
      "Epoch [13/100], Loss: 0.0118\n",
      "Epoch [14/100], Loss: 0.0109\n",
      "Epoch [15/100], Loss: 0.0110\n",
      "Epoch [16/100], Loss: 0.0093\n",
      "Epoch [17/100], Loss: 0.0092\n",
      "Epoch [18/100], Loss: 0.0086\n",
      "Epoch [19/100], Loss: 0.0068\n",
      "Epoch [20/100], Loss: 0.0077\n",
      "Epoch [21/100], Loss: 0.0084\n",
      "Epoch [22/100], Loss: 0.0057\n",
      "Epoch [23/100], Loss: 0.0060\n",
      "Epoch [24/100], Loss: 0.0058\n",
      "Epoch [25/100], Loss: 0.0062\n",
      "Epoch [26/100], Loss: 0.0058\n",
      "Epoch [27/100], Loss: 0.0049\n",
      "Epoch [28/100], Loss: 0.0057\n",
      "Epoch [29/100], Loss: 0.0038\n",
      "Epoch [30/100], Loss: 0.0034\n",
      "Epoch [31/100], Loss: 0.0060\n",
      "Epoch [32/100], Loss: 0.0035\n",
      "Epoch [33/100], Loss: 0.0026\n",
      "Epoch [34/100], Loss: 0.0068\n",
      "Epoch [35/100], Loss: 0.0018\n",
      "Epoch [36/100], Loss: 0.0024\n",
      "Epoch [37/100], Loss: 0.0047\n",
      "Epoch [38/100], Loss: 0.0042\n",
      "Epoch [39/100], Loss: 0.0011\n",
      "Epoch [40/100], Loss: 0.0051\n",
      "Epoch [41/100], Loss: 0.0019\n",
      "Epoch [42/100], Loss: 0.0061\n",
      "Epoch [43/100], Loss: 0.0039\n",
      "Epoch [44/100], Loss: 0.0022\n",
      "Epoch [45/100], Loss: 0.0047\n",
      "Epoch [46/100], Loss: 0.0016\n",
      "Epoch [47/100], Loss: 0.0037\n",
      "Epoch [48/100], Loss: 0.0026\n",
      "Epoch [49/100], Loss: 0.0024\n",
      "Epoch [50/100], Loss: 0.0024\n",
      "Epoch [51/100], Loss: 0.0001\n",
      "Epoch [52/100], Loss: 0.0066\n",
      "Epoch [53/100], Loss: 0.0026\n",
      "Epoch [54/100], Loss: 0.0001\n",
      "Epoch [55/100], Loss: 0.0072\n",
      "Epoch [56/100], Loss: 0.0012\n",
      "Epoch [57/100], Loss: 0.0025\n",
      "Epoch [58/100], Loss: 0.0002\n",
      "Epoch [59/100], Loss: 0.0031\n",
      "Epoch [60/100], Loss: 0.0026\n",
      "Epoch [61/100], Loss: 0.0014\n",
      "Epoch [62/100], Loss: 0.0021\n",
      "Epoch [63/100], Loss: 0.0025\n",
      "Epoch [64/100], Loss: 0.0002\n",
      "Epoch [65/100], Loss: 0.0001\n",
      "Epoch [66/100], Loss: 0.0088\n",
      "Epoch [67/100], Loss: 0.0002\n",
      "Epoch [68/100], Loss: 0.0001\n",
      "Epoch [69/100], Loss: 0.0001\n",
      "Epoch [70/100], Loss: 0.0010\n",
      "Epoch [71/100], Loss: 0.0116\n",
      "Epoch [72/100], Loss: 0.0005\n",
      "Epoch [73/100], Loss: 0.0005\n",
      "Epoch [74/100], Loss: 0.0001\n",
      "Epoch [75/100], Loss: 0.0058\n",
      "Epoch [76/100], Loss: 0.0044\n",
      "Epoch [77/100], Loss: 0.0014\n",
      "Epoch [78/100], Loss: 0.0001\n",
      "Epoch [79/100], Loss: 0.0001\n",
      "Epoch [80/100], Loss: 0.0007\n",
      "Epoch [81/100], Loss: 0.0000\n",
      "Epoch [82/100], Loss: 0.0078\n",
      "Epoch [83/100], Loss: 0.0011\n",
      "Epoch [84/100], Loss: 0.0002\n",
      "Epoch [85/100], Loss: 0.0053\n",
      "Epoch [86/100], Loss: 0.0010\n",
      "Epoch [87/100], Loss: 0.0003\n",
      "Epoch [88/100], Loss: 0.0002\n",
      "Epoch [89/100], Loss: 0.0000\n",
      "Epoch [90/100], Loss: 0.0062\n",
      "Epoch [91/100], Loss: 0.0012\n",
      "Epoch [92/100], Loss: 0.0012\n",
      "Epoch [93/100], Loss: 0.0009\n",
      "Epoch [94/100], Loss: 0.0001\n",
      "Epoch [95/100], Loss: 0.0000\n",
      "Epoch [96/100], Loss: 0.0000\n",
      "Epoch [97/100], Loss: 0.0000\n",
      "Epoch [98/100], Loss: 0.0037\n",
      "Epoch [99/100], Loss: 0.0084\n",
      "Epoch [100/100], Loss: 0.0020\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from spuco.datasets import SpuCoMNIST, SpuriousFeatureDifficulty\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define classes and spurious feature difficulty\n",
    "classes = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
    "difficulty = SpuriousFeatureDifficulty.MAGNITUDE_LARGE\n",
    "\n",
    "root_dir = \"./mnist_data/\" \n",
    "\n",
    "# Initialize the train dataset\n",
    "trainset = SpuCoMNIST(\n",
    "    root=root_dir,\n",
    "    spurious_feature_difficulty=difficulty,\n",
    "    spurious_correlation_strength=0.995,\n",
    "    classes=classes,\n",
    "    split=\"train\"\n",
    ")\n",
    "trainset.initialize()\n",
    "\n",
    "# Initialize the test dataset\n",
    "testset = SpuCoMNIST(\n",
    "    root=root_dir,\n",
    "    spurious_feature_difficulty=difficulty,\n",
    "    classes=classes,\n",
    "    split=\"test\"\n",
    ")\n",
    "testset.initialize()\n",
    "\n",
    "# Dataloader\n",
    "train_loader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3 * 28 * 28, 128)  # For RGB images with size 28x28\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define train function\n",
    "def train(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in loader:  # Expect only 2 values: images and labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "model = SimpleNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model with ERM\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer)\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {train_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d7be925-5b2e-44ee-b9ed-f335fc243c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openblas64__info:\n",
      "    libraries = ['openblas64_', 'openblas64_']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]\n",
      "    runtime_library_dirs = ['/usr/local/lib']\n",
      "blas_ilp64_opt_info:\n",
      "    libraries = ['openblas64_', 'openblas64_']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None)]\n",
      "    runtime_library_dirs = ['/usr/local/lib']\n",
      "openblas64__lapack_info:\n",
      "    libraries = ['openblas64_', 'openblas64_']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]\n",
      "    runtime_library_dirs = ['/usr/local/lib']\n",
      "lapack_ilp64_opt_info:\n",
      "    libraries = ['openblas64_', 'openblas64_']\n",
      "    library_dirs = ['/usr/local/lib']\n",
      "    language = c\n",
      "    define_macros = [('HAVE_CBLAS', None), ('BLAS_SYMBOL_SUFFIX', '64_'), ('HAVE_BLAS_ILP64', None), ('HAVE_LAPACKE', None)]\n",
      "    runtime_library_dirs = ['/usr/local/lib']\n",
      "Supported SIMD extensions in this NumPy install:\n",
      "    baseline = NEON,NEON_FP16,NEON_VFPV4,ASIMD\n",
      "    found = ASIMDHP,ASIMDDP\n",
      "    not found = ASIMDFHM\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "np.__config__.show()\n",
    "\n",
    "# Faulty threadpool fix\n",
    "from threadpoolctl import threadpool_limits\n",
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# Extract embeddings from model\n",
    "def extract_embeddings(model, loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    embeddings = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for images, labels in loader:\n",
    "            # Flatten the images\n",
    "            x = images.view(images.size(0), -1) \n",
    "            embedding = model.fc1(x)  \n",
    "            embeddings.append(embedding.cpu()) \n",
    "            labels_list.append(labels.cpu()) \n",
    "    return torch.cat(embeddings), torch.cat(labels_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5db5f3ff-3d7d-433e-bfa7-34fbaa1061c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48004, 128)\n"
     ]
    }
   ],
   "source": [
    "# Extract embeddings from the trained model\n",
    "train_embeddings, train_labels = extract_embeddings(model, train_loader)\n",
    "\n",
    "# Convert embeddings to NumPy format for KMeans clustering\n",
    "train_embeddings_np = train_embeddings.numpy()  # Convert to NumPy array\n",
    "print(train_embeddings_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0423a134-9fa5-4a99-b1b6-c9beff2a4f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the dataset: 48004\n"
     ]
    }
   ],
   "source": [
    "# Check the number of samples in the dataset\n",
    "num_samples_in_dataset = len(train_loader.dataset)\n",
    "print(f\"Number of samples in the dataset: {num_samples_in_dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8ee59b9-0f5a-47a1-b6af-f4189e2efbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train_embeddings: 48004\n"
     ]
    }
   ],
   "source": [
    "num_samples = train_embeddings.shape[0]\n",
    "print(\"Number of samples in train_embeddings:\", num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e452813-16c8-4010-8162-a300f8376803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster assignments: [1 2 0 ... 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "# K-Means clustering with threadpool control\n",
    "n_clusters = 3  # \n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=0).fit(train_embeddings_np)\n",
    "\n",
    "# Assign each sample to a cluster\n",
    "train_clusters = kmeans.labels_\n",
    "\n",
    "# Print the cluster assignments\n",
    "print(\"Cluster assignments:\", train_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94962608-fd0e-4963-a4af-090bb57c6be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# Create group-wise datasets\n",
    "# Use the cluster assignments from K-Means\n",
    "train_groups = kmeans.labels_\n",
    "\n",
    "# Create group-wise datasets\n",
    "group_indices = [[] for _ in range(n_clusters)]\n",
    "for idx, group in enumerate(train_groups):\n",
    "    group_indices[group].append(idx)\n",
    "\n",
    "group_dataloaders = [DataLoader(Subset(trainset, indices), batch_size=64, shuffle=True) for indices in group_indices]\n",
    "\n",
    "# Balanced group training loop\n",
    "import itertools\n",
    "\n",
    "def balanced_train(model, group_dataloaders, criterion, optimizer, max_steps):\n",
    "    model.train()\n",
    "    group_iters = [iter(loader) for loader in group_dataloaders]\n",
    "    group_cycle = itertools.cycle(enumerate(group_iters))\n",
    "    \n",
    "    steps = 0\n",
    "    while steps < max_steps:\n",
    "        try:\n",
    "            i, group_iter = next(group_cycle)\n",
    "            images, labels = next(group_iter)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            steps += 1\n",
    "        except StopIteration:\n",
    "            # Reset the exhausted iterator\n",
    "            group_iters[i] = iter(group_dataloaders[i])\n",
    "            continue\n",
    "\n",
    "# Retrain using group-balanced batches\n",
    "max_steps_per_epoch = len(trainset) // 64  # Adjust based on batch size\n",
    "for epoch in range(epochs):\n",
    "    balanced_train(model, group_dataloaders, criterion, optimizer, max_steps=max_steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97fb2c5f-1fbb-4672-8e69-c72935200316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Test Loss: 10.0623, Test Accuracy: 37.80%\n",
      "Epoch [2/100], Test Loss: 10.1812, Test Accuracy: 38.31%\n",
      "Epoch [3/100], Test Loss: 10.3076, Test Accuracy: 37.30%\n",
      "Epoch [4/100], Test Loss: 10.2932, Test Accuracy: 38.56%\n",
      "Epoch [5/100], Test Loss: 10.3994, Test Accuracy: 38.30%\n",
      "Epoch [6/100], Test Loss: 10.3980, Test Accuracy: 38.86%\n",
      "Epoch [7/100], Test Loss: 10.8612, Test Accuracy: 38.11%\n",
      "Epoch [8/100], Test Loss: 10.6602, Test Accuracy: 39.14%\n",
      "Epoch [9/100], Test Loss: 10.4847, Test Accuracy: 39.04%\n",
      "Epoch [10/100], Test Loss: 10.5360, Test Accuracy: 39.29%\n",
      "Epoch [11/100], Test Loss: 10.8167, Test Accuracy: 38.73%\n",
      "Epoch [12/100], Test Loss: 10.7121, Test Accuracy: 39.30%\n",
      "Epoch [13/100], Test Loss: 10.9572, Test Accuracy: 38.56%\n",
      "Epoch [14/100], Test Loss: 10.9274, Test Accuracy: 39.08%\n",
      "Epoch [15/100], Test Loss: 11.0578, Test Accuracy: 38.60%\n",
      "Epoch [16/100], Test Loss: 11.1447, Test Accuracy: 38.88%\n",
      "Epoch [17/100], Test Loss: 11.3127, Test Accuracy: 38.92%\n",
      "Epoch [18/100], Test Loss: 11.3216, Test Accuracy: 39.14%\n",
      "Epoch [19/100], Test Loss: 11.3406, Test Accuracy: 39.36%\n",
      "Epoch [20/100], Test Loss: 11.4294, Test Accuracy: 38.67%\n",
      "Epoch [21/100], Test Loss: 11.6998, Test Accuracy: 40.26%\n",
      "Epoch [22/100], Test Loss: 11.1302, Test Accuracy: 41.21%\n",
      "Epoch [23/100], Test Loss: 11.2696, Test Accuracy: 40.75%\n",
      "Epoch [24/100], Test Loss: 11.4205, Test Accuracy: 40.56%\n",
      "Epoch [25/100], Test Loss: 11.4575, Test Accuracy: 40.58%\n",
      "Epoch [26/100], Test Loss: 11.5542, Test Accuracy: 40.50%\n",
      "Epoch [27/100], Test Loss: 11.6592, Test Accuracy: 40.64%\n",
      "Epoch [28/100], Test Loss: 11.7278, Test Accuracy: 40.36%\n",
      "Epoch [29/100], Test Loss: 11.8347, Test Accuracy: 40.35%\n",
      "Epoch [30/100], Test Loss: 11.9364, Test Accuracy: 40.13%\n",
      "Epoch [31/100], Test Loss: 12.0625, Test Accuracy: 39.74%\n",
      "Epoch [32/100], Test Loss: 14.1304, Test Accuracy: 36.85%\n",
      "Epoch [33/100], Test Loss: 11.6853, Test Accuracy: 38.60%\n",
      "Epoch [34/100], Test Loss: 12.6373, Test Accuracy: 37.36%\n",
      "Epoch [35/100], Test Loss: 11.8021, Test Accuracy: 39.12%\n",
      "Epoch [36/100], Test Loss: 11.7795, Test Accuracy: 39.19%\n",
      "Epoch [37/100], Test Loss: 11.8300, Test Accuracy: 39.24%\n",
      "Epoch [38/100], Test Loss: 11.8435, Test Accuracy: 39.32%\n",
      "Epoch [39/100], Test Loss: 11.9793, Test Accuracy: 39.28%\n",
      "Epoch [40/100], Test Loss: 12.0515, Test Accuracy: 39.16%\n",
      "Epoch [41/100], Test Loss: 12.2350, Test Accuracy: 39.02%\n",
      "Epoch [42/100], Test Loss: 11.8948, Test Accuracy: 39.23%\n",
      "Epoch [43/100], Test Loss: 12.4148, Test Accuracy: 38.93%\n",
      "Epoch [44/100], Test Loss: 12.1383, Test Accuracy: 39.52%\n",
      "Epoch [45/100], Test Loss: 12.2310, Test Accuracy: 39.07%\n",
      "Epoch [46/100], Test Loss: 12.1727, Test Accuracy: 39.32%\n",
      "Epoch [47/100], Test Loss: 12.1435, Test Accuracy: 39.34%\n",
      "Epoch [48/100], Test Loss: 12.1872, Test Accuracy: 39.34%\n",
      "Epoch [49/100], Test Loss: 12.1686, Test Accuracy: 39.69%\n",
      "Epoch [50/100], Test Loss: 12.0702, Test Accuracy: 39.89%\n",
      "Epoch [51/100], Test Loss: 12.2339, Test Accuracy: 39.40%\n",
      "Epoch [52/100], Test Loss: 20.0541, Test Accuracy: 30.51%\n",
      "Epoch [53/100], Test Loss: 10.8951, Test Accuracy: 41.15%\n",
      "Epoch [54/100], Test Loss: 11.0718, Test Accuracy: 40.54%\n",
      "Epoch [55/100], Test Loss: 11.9221, Test Accuracy: 39.82%\n",
      "Epoch [56/100], Test Loss: 12.2919, Test Accuracy: 39.20%\n",
      "Epoch [57/100], Test Loss: 12.3116, Test Accuracy: 39.08%\n",
      "Epoch [58/100], Test Loss: 12.2898, Test Accuracy: 39.35%\n",
      "Epoch [59/100], Test Loss: 12.4359, Test Accuracy: 38.89%\n",
      "Epoch [60/100], Test Loss: 12.3915, Test Accuracy: 39.27%\n",
      "Epoch [61/100], Test Loss: 12.6240, Test Accuracy: 38.81%\n",
      "Epoch [62/100], Test Loss: 12.7657, Test Accuracy: 38.48%\n",
      "Epoch [63/100], Test Loss: 12.7394, Test Accuracy: 38.83%\n",
      "Epoch [64/100], Test Loss: 12.6897, Test Accuracy: 39.36%\n",
      "Epoch [65/100], Test Loss: 15.8470, Test Accuracy: 35.06%\n",
      "Epoch [66/100], Test Loss: 12.6674, Test Accuracy: 38.10%\n",
      "Epoch [67/100], Test Loss: 12.5529, Test Accuracy: 39.08%\n",
      "Epoch [68/100], Test Loss: 12.5946, Test Accuracy: 39.08%\n",
      "Epoch [69/100], Test Loss: 12.7176, Test Accuracy: 38.80%\n",
      "Epoch [70/100], Test Loss: 12.8040, Test Accuracy: 38.66%\n",
      "Epoch [71/100], Test Loss: 12.9280, Test Accuracy: 38.57%\n",
      "Epoch [72/100], Test Loss: 12.8680, Test Accuracy: 38.66%\n",
      "Epoch [73/100], Test Loss: 13.0427, Test Accuracy: 38.27%\n",
      "Epoch [74/100], Test Loss: 13.0260, Test Accuracy: 38.43%\n",
      "Epoch [75/100], Test Loss: 13.2351, Test Accuracy: 38.08%\n",
      "Epoch [76/100], Test Loss: 17.6572, Test Accuracy: 33.91%\n",
      "Epoch [77/100], Test Loss: 14.5582, Test Accuracy: 36.74%\n",
      "Epoch [78/100], Test Loss: 13.1443, Test Accuracy: 39.15%\n",
      "Epoch [79/100], Test Loss: 13.1465, Test Accuracy: 39.20%\n",
      "Epoch [80/100], Test Loss: 13.0904, Test Accuracy: 39.39%\n",
      "Epoch [81/100], Test Loss: 13.0712, Test Accuracy: 39.46%\n",
      "Epoch [82/100], Test Loss: 13.0155, Test Accuracy: 39.57%\n",
      "Epoch [83/100], Test Loss: 12.9945, Test Accuracy: 39.70%\n",
      "Epoch [84/100], Test Loss: 13.0352, Test Accuracy: 39.84%\n",
      "Epoch [85/100], Test Loss: 13.0012, Test Accuracy: 39.83%\n",
      "Epoch [86/100], Test Loss: 13.1192, Test Accuracy: 39.69%\n",
      "Epoch [87/100], Test Loss: 12.9253, Test Accuracy: 40.21%\n",
      "Epoch [88/100], Test Loss: 13.0553, Test Accuracy: 39.79%\n",
      "Epoch [89/100], Test Loss: 16.4778, Test Accuracy: 36.57%\n",
      "Epoch [90/100], Test Loss: 13.4979, Test Accuracy: 41.60%\n",
      "Epoch [91/100], Test Loss: 14.3364, Test Accuracy: 39.01%\n",
      "Epoch [92/100], Test Loss: 13.6279, Test Accuracy: 39.91%\n",
      "Epoch [93/100], Test Loss: 13.5722, Test Accuracy: 39.87%\n",
      "Epoch [94/100], Test Loss: 13.5067, Test Accuracy: 39.91%\n",
      "Epoch [95/100], Test Loss: 13.4767, Test Accuracy: 39.93%\n",
      "Epoch [96/100], Test Loss: 13.4728, Test Accuracy: 39.83%\n",
      "Epoch [97/100], Test Loss: 13.5260, Test Accuracy: 39.60%\n",
      "Epoch [98/100], Test Loss: 13.5816, Test Accuracy: 39.43%\n",
      "Epoch [99/100], Test Loss: 13.5412, Test Accuracy: 39.55%\n",
      "Epoch [100/100], Test Loss: 13.5570, Test Accuracy: 39.51%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return total_loss / len(loader), accuracy\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    balanced_train(model, group_dataloaders, criterion, optimizer, max_steps=max_steps_per_epoch)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader, criterion)\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6d29349-4322-4792-82bb-e9e2ae8296e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook path: /Users/andrewsuh/Desktop/Models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "notebook_path = os.path.abspath(\"\")\n",
    "print(\"Notebook path:\", notebook_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca31e9e-8a1d-4f83-8383-c87bb4e0684f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3cdb9f-8f7b-4853-be29-61329b81e114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
